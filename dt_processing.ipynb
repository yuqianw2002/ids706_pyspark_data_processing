{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f39278f",
   "metadata": {},
   "source": [
    "# PM2.5 Dataset from 2021 to 2023 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfe2a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, avg, max, min, count, sum as spark_sum,\n",
    "    year, month, dayofweek, when, lit, round as spark_round,\n",
    "    greatest, coalesce, dense_rank, row_number\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, \n",
    "    StringType, IntegerType, DoubleType, DateType\n",
    ")\n",
    "import time\n",
    "\n",
    "# Start Spark\n",
    "# Most basic Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PM25\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109d9c26",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ad9c694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+--------------+---+---------+----------+-----+--------------------+---------------+------------------+----------+--------------------+----------+-----------------+-------------------+---------------+-------------+------------+---+-----------+--------------------+-----------------+--------------------+----------+-----------+---------+--------------------+-------------------+\n",
      "|State Code|County Code|Site Num|Parameter Code|POC| Latitude| Longitude|Datum|      Parameter Name|Sample Duration|Pollutant Standard|Date Local|    Units of Measure|Event Type|Observation Count|Observation Percent|Arithmetic Mean|1st Max Value|1st Max Hour|AQI|Method Code|         Method Name|  Local Site Name|             Address|State Name|County Name|City Name|           CBSA Name|Date of Last Change|\n",
      "+----------+-----------+--------+--------------+---+---------+----------+-----+--------------------+---------------+------------------+----------+--------------------+----------+-----------------+-------------------+---------------+-------------+------------+---+-----------+--------------------+-----------------+--------------------+----------+-----------+---------+--------------------+-------------------+\n",
      "|        01|        003|    0010|         88101|  1|30.497478|-87.880258|NAD83|PM2.5 - Local Con...|        24 HOUR| PM25 24-hour 2012|2021-01-01|Micrograms/cubic ...|      None|                1|              100.0|            6.4|          6.4|           0| 36|        145|R & P Model 2025 ...|FAIRHOPE, Alabama|FAIRHOPE HIGH SCH...|   Alabama|    Baldwin| Fairhope|Daphne-Fairhope-F...|         2024-10-16|\n",
      "|        01|        003|    0010|         88101|  1|30.497478|-87.880258|NAD83|PM2.5 - Local Con...|        24 HOUR| PM25 24-hour 2012|2021-01-07|Micrograms/cubic ...|      None|                1|              100.0|            5.7|          5.7|           0| 32|        145|R & P Model 2025 ...|FAIRHOPE, Alabama|FAIRHOPE HIGH SCH...|   Alabama|    Baldwin| Fairhope|Daphne-Fairhope-F...|         2024-10-16|\n",
      "|        01|        003|    0010|         88101|  1|30.497478|-87.880258|NAD83|PM2.5 - Local Con...|        24 HOUR| PM25 24-hour 2012|2021-01-13|Micrograms/cubic ...|      None|                1|              100.0|             11|           11|           0| 55|        145|R & P Model 2025 ...|FAIRHOPE, Alabama|FAIRHOPE HIGH SCH...|   Alabama|    Baldwin| Fairhope|Daphne-Fairhope-F...|         2024-10-16|\n",
      "|        01|        003|    0010|         88101|  1|30.497478|-87.880258|NAD83|PM2.5 - Local Con...|        24 HOUR| PM25 24-hour 2012|2021-01-16|Micrograms/cubic ...|      None|                1|              100.0|            5.1|          5.1|           0| 28|        145|R & P Model 2025 ...|FAIRHOPE, Alabama|FAIRHOPE HIGH SCH...|   Alabama|    Baldwin| Fairhope|Daphne-Fairhope-F...|         2024-10-16|\n",
      "|        01|        003|    0010|         88101|  1|30.497478|-87.880258|NAD83|PM2.5 - Local Con...|        24 HOUR| PM25 24-hour 2012|2021-01-19|Micrograms/cubic ...|      None|                1|              100.0|           12.7|         12.7|           0| 58|        145|R & P Model 2025 ...|FAIRHOPE, Alabama|FAIRHOPE HIGH SCH...|   Alabama|    Baldwin| Fairhope|Daphne-Fairhope-F...|         2024-10-16|\n",
      "+----------+-----------+--------+--------------+---+---------+----------+-----+--------------------+---------------+------------------+----------+--------------------+----------+-----------------+-------------------+---------------+-------------+------------+---+-----------+--------------------+-----------------+--------------------+----------+-----------+---------+--------------------+-------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 23:36:10 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: data/epa_raw/daily_88101_*.csv.\n",
      "java.io.FileNotFoundException: File data/epa_raw/daily_88101_*.csv does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:392)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:259)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    }
   ],
   "source": [
    "# Load PM2.5 data\n",
    "pm25_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"data/epa_raw/daily_88101_*.csv\")\n",
    "\n",
    "pm25_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1b9a7b",
   "metadata": {},
   "source": [
    "## Transformation type, filter, Join and group by operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6517b549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+-----------+--------+--------------+---+---------+----------+-----+--------------------+---------------+------------------+----------+--------------------+----------+-----------------+-------------------+---------------+-------------+------------+---+-----------+--------------------+-----------------+--------------------+-----------+--------------------+-------------------+----------+------------+-----+---------+-----------------+-----------------+\n",
      "|State Name|City Name|State Code|County Code|Site Num|Parameter Code|POC| Latitude| Longitude|Datum|      Parameter Name|Sample Duration|Pollutant Standard|Date Local|    Units of Measure|Event Type|Observation Count|Observation Percent|Arithmetic Mean|1st Max Value|1st Max Hour|AQI|Method Code|         Method Name|  Local Site Name|             Address|County Name|           CBSA Name|Date of Last Change|date_local|arith_mean_d|aqi_i|obs_pct_d|measurement_count|         city_avg|\n",
      "+----------+---------+----------+-----------+--------+--------------+---+---------+----------+-----+--------------------+---------------+------------------+----------+--------------------+----------+-----------------+-------------------+---------------+-------------+------------+---+-----------+--------------------+-----------------+--------------------+-----------+--------------------+-------------------+----------+------------+-----+---------+-----------------+-----------------+\n",
      "|   Alabama| Fairhope|        01|        003|    0010|         88101|  1|30.497478|-87.880258|NAD83|PM2.5 - Local Con...|        24 HOUR| PM25 24-hour 2012|2021-01-01|Micrograms/cubic ...|      None|                1|              100.0|            6.4|          6.4|           0| 36|        145|R & P Model 2025 ...|FAIRHOPE, Alabama|FAIRHOPE HIGH SCH...|    Baldwin|Daphne-Fairhope-F...|         2024-10-16|2021-01-01|         6.4|   36|    100.0|              567|7.488007054673719|\n",
      "|   Alabama| Fairhope|        01|        003|    0010|         88101|  1|30.497478|-87.880258|NAD83|PM2.5 - Local Con...|        24 HOUR| PM25 24-hour 2012|2021-01-07|Micrograms/cubic ...|      None|                1|              100.0|            5.7|          5.7|           0| 32|        145|R & P Model 2025 ...|FAIRHOPE, Alabama|FAIRHOPE HIGH SCH...|    Baldwin|Daphne-Fairhope-F...|         2024-10-16|2021-01-07|         5.7|   32|    100.0|              567|7.488007054673719|\n",
      "|   Alabama| Fairhope|        01|        003|    0010|         88101|  1|30.497478|-87.880258|NAD83|PM2.5 - Local Con...|        24 HOUR| PM25 24-hour 2012|2021-01-13|Micrograms/cubic ...|      None|                1|              100.0|             11|           11|           0| 55|        145|R & P Model 2025 ...|FAIRHOPE, Alabama|FAIRHOPE HIGH SCH...|    Baldwin|Daphne-Fairhope-F...|         2024-10-16|2021-01-13|        11.0|   55|    100.0|              567|7.488007054673719|\n",
      "|   Alabama| Fairhope|        01|        003|    0010|         88101|  1|30.497478|-87.880258|NAD83|PM2.5 - Local Con...|        24 HOUR| PM25 24-hour 2012|2021-01-16|Micrograms/cubic ...|      None|                1|              100.0|            5.1|          5.1|           0| 28|        145|R & P Model 2025 ...|FAIRHOPE, Alabama|FAIRHOPE HIGH SCH...|    Baldwin|Daphne-Fairhope-F...|         2024-10-16|2021-01-16|         5.1|   28|    100.0|              567|7.488007054673719|\n",
      "|   Alabama| Fairhope|        01|        003|    0010|         88101|  1|30.497478|-87.880258|NAD83|PM2.5 - Local Con...|        24 HOUR| PM25 24-hour 2012|2021-01-19|Micrograms/cubic ...|      None|                1|              100.0|           12.7|         12.7|           0| 58|        145|R & P Model 2025 ...|FAIRHOPE, Alabama|FAIRHOPE HIGH SCH...|    Baldwin|Daphne-Fairhope-F...|         2024-10-16|2021-01-19|        12.7|   58|    100.0|              567|7.488007054673719|\n",
      "+----------+---------+----------+-----------+--------+--------------+---+---------+----------+-----+--------------------+---------------+------------------+----------+--------------------+----------+-----------------+-------------------+---------------+-------------+------------+---+-----------+--------------------+-----------------+--------------------+-----------+--------------------+-------------------+----------+------------+-----+---------+-----------------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# filtering PM2.5 data\n",
    "pm25_filtered = pm25_df\n",
    "\n",
    "# column type casting and transformation\n",
    "pm25_typed = (pm25_filtered\n",
    "    .withColumn(\"date_local\", F.to_date(col(\"Date Local\")))              \n",
    "    .withColumn(\"arith_mean_d\", col(\"Arithmetic Mean\").cast(\"double\"))    \n",
    "    .withColumn(\"aqi_i\",        col(\"AQI\").cast(\"int\"))                  \n",
    "    .withColumn(\"obs_pct_d\",    col(\"Observation Percent\").cast(\"double\"))\n",
    ")\n",
    "\n",
    "pm25_filtered = (pm25_typed\n",
    "    .filter(col(\"date_local\") >= F.lit(\"2021-01-01\").cast(\"date\"))   \n",
    "    .filter(col(\"arith_mean_d\").isNotNull() &\n",
    "            (col(\"arith_mean_d\") >= 0) &\n",
    "            (col(\"arith_mean_d\") < 500))                            \n",
    "    .filter(col(\"aqi_i\").isNotNull())\n",
    "    .filter(col(\"obs_pct_d\") >= F.lit(75.0))                         \n",
    "    .filter(col(\"CBSA Name\").isNotNull() & col(\"City Name\").isNotNull())\n",
    ")\n",
    "\n",
    "# join to find stations in the sma city\n",
    "city_stations = pm25_filtered.groupBy(\"State Name\", \"City Name\").agg(\n",
    "    count(\"*\").alias(\"measurement_count\"),\n",
    "    avg(\"Arithmetic Mean\").alias(\"city_avg\")\n",
    ")\n",
    "\n",
    "result_df = pm25_filtered.join(city_stations, [\"State Name\", \"City Name\"], \"left\")\n",
    "\n",
    "result_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e76e72c",
   "metadata": {},
   "source": [
    "## Column transformation with Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd3b3dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations to enrich data\n",
    "enriched_df = result_df \\\n",
    "    .withColumn(\"Year\", year(col(\"Date Local\"))) \\\n",
    "    .withColumn(\"Month\", month(col(\"Date Local\"))) \\\n",
    "    .withColumn(\"Season\",\n",
    "        when(col(\"Month\").isin([12, 1, 2]), \"Winter\")\n",
    "        .when(col(\"Month\").isin([3, 4, 5]), \"Spring\")\n",
    "        .when(col(\"Month\").isin([6, 7, 8]), \"Summer\")\n",
    "        .otherwise(\"Fall\")\n",
    "    ) \\\n",
    "    .withColumn(\"AQI_Category\",\n",
    "        when(col(\"AQI\") <= 50, \"Good\")\n",
    "        .when(col(\"AQI\") <= 100, \"Moderate\")\n",
    "        .when(col(\"AQI\") <= 150, \"Unhealthy for Sensitive\")\n",
    "        .otherwise(\"Unhealthy\")\n",
    "    ) \\\n",
    "    .withColumn(\"PM25_Rounded\", spark_round(col(\"Arithmetic Mean\"), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e0f3e",
   "metadata": {},
   "source": [
    "## SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0cae5d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+----+\n",
      "|State Name|  City Name|Avg_PM25|Days|\n",
      "+----------+-----------+--------+----+\n",
      "|California|Bakersfield|   15.56|2471|\n",
      "|California|    Visalia|   15.55|1050|\n",
      "|    Oregon|   Oakridge|   14.24|1133|\n",
      "|California|    Hanford|   14.13|1075|\n",
      "|California|    Ontario|   13.99|1938|\n",
      "|California|     Fresno|   13.66|3473|\n",
      "|California|   Corcoran|   13.13|1070|\n",
      "|California|  Otay Mesa|   12.98| 788|\n",
      "|California|    Modesto|   12.96|1064|\n",
      "|California|  Mira Loma|   12.92|2291|\n",
      "+----------+-----------+--------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# df table\n",
    "enriched_df.createOrReplaceTempView(\"pm25_data\")\n",
    "\n",
    "# 1. Top polluted cities\n",
    "# Fixed SQL query\n",
    "query1 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        `State Name`,\n",
    "        `City Name`,\n",
    "        ROUND(AVG(`Arithmetic Mean`), 2) as Avg_PM25,\n",
    "        COUNT(*) as Days\n",
    "    FROM pm25_data\n",
    "    GROUP BY `State Name`, `City Name`\n",
    "    HAVING COUNT(*) >= 100\n",
    "    ORDER BY Avg_PM25 DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "query1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cde81e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+------------+\n",
      "|Year|Month|Monthly_Avg|Measurements|\n",
      "+----+-----+-----------+------------+\n",
      "|2021|    1|       8.72|       32534|\n",
      "|2021|    2|       8.52|       28847|\n",
      "|2021|    3|       7.38|       31524|\n",
      "|2021|    4|       7.65|       31928|\n",
      "|2021|    5|        6.9|       33238|\n",
      "|2021|    6|       7.49|       32120|\n",
      "|2021|    7|      11.53|       33642|\n",
      "|2021|    8|      11.79|       34049|\n",
      "|2021|    9|       8.46|       33130|\n",
      "|2021|   10|        6.8|       34749|\n",
      "|2021|   11|       8.09|       33589|\n",
      "|2021|   12|       8.56|       34837|\n",
      "|2022|    1|       8.89|       35093|\n",
      "|2022|    2|       8.02|       32188|\n",
      "|2022|    3|       6.98|       35398|\n",
      "|2022|    4|       6.57|       34553|\n",
      "|2022|    5|       7.03|       35985|\n",
      "|2022|    6|       8.01|       35230|\n",
      "|2022|    7|       7.71|       36320|\n",
      "|2022|    8|       6.79|       36299|\n",
      "+----+-----+-----------+------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 2. Monthly trends\n",
    "query2 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Year,\n",
    "        Month,\n",
    "        ROUND(AVG(`Arithmetic Mean`), 2) as Monthly_Avg,\n",
    "        COUNT(*) as Measurements\n",
    "    FROM pm25_data\n",
    "    GROUP BY Year, Month\n",
    "    ORDER BY Year, Month\n",
    "\"\"\")\n",
    "query2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e062b2",
   "metadata": {},
   "source": [
    "## Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a50ae1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 23:54:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Only keep columns we actually use\n",
    "enriched_df = pm25_filtered.select(\n",
    "    \"State Code\",\n",
    "    \"County Code\", \n",
    "    \"Site Num\",\n",
    "    \"Date Local\",\n",
    "    \"Arithmetic Mean\",\n",
    "    \"AQI\",\n",
    "    \"State Name\",\n",
    "    \"City Name\"\n",
    ")\n",
    "\n",
    "enriched_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"output/pm25_enriched\")\n",
    "\n",
    "query1.write.mode(\"overwrite\").parquet(\"output/top_cities\")\n",
    "query2.write.mode(\"overwrite\").parquet(\"output/monthly_trends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5933649e",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8168115c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* Project (3)\n",
      "+- * Filter (2)\n",
      "   +- Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [10]: [State Code#3112, County Code#3113, Site Num#3114, Date Local#3123, Observation Percent#3127, Arithmetic Mean#3128, AQI#3131, State Name#3136, City Name#3138, CBSA Name#3139]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/Users/yuqianwang/Documents/IDS706/ids706_pyspark_data_processing/data/epa_raw/daily_88101_2021.csv, ... 2 entries]\n",
      "PushedFilters: [IsNotNull(Date Local), IsNotNull(Arithmetic Mean), IsNotNull(AQI), IsNotNull(Observation Percent), IsNotNull(CBSA Name), IsNotNull(City Name)]\n",
      "ReadSchema: struct<State Code:string,County Code:string,Site Num:string,Date Local:string,Observation Percent:string,Arithmetic Mean:string,AQI:string,State Name:string,City Name:string,CBSA Name:string>\n",
      "\n",
      "(2) Filter [codegen id : 1]\n",
      "Input [10]: [State Code#3112, County Code#3113, Site Num#3114, Date Local#3123, Observation Percent#3127, Arithmetic Mean#3128, AQI#3131, State Name#3136, City Name#3138, CBSA Name#3139]\n",
      "Condition : (((((((((((isnotnull(Date Local#3123) AND isnotnull(Arithmetic Mean#3128)) AND isnotnull(AQI#3131)) AND isnotnull(Observation Percent#3127)) AND (cast(Date Local#3123 as date) >= 2021-01-01)) AND isnotnull(cast(Arithmetic Mean#3128 as double))) AND (cast(Arithmetic Mean#3128 as double) >= 0.0)) AND (cast(Arithmetic Mean#3128 as double) < 500.0)) AND isnotnull(cast(AQI#3131 as int))) AND (cast(Observation Percent#3127 as double) >= 75.0)) AND isnotnull(CBSA Name#3139)) AND isnotnull(City Name#3138))\n",
      "\n",
      "(3) Project [codegen id : 1]\n",
      "Output [8]: [State Code#3112, County Code#3113, Site Num#3114, Date Local#3123, Arithmetic Mean#3128, AQI#3131, State Name#3136, City Name#3138]\n",
      "Input [10]: [State Code#3112, County Code#3113, Site Num#3114, Date Local#3123, Observation Percent#3127, Arithmetic Mean#3128, AQI#3131, State Name#3136, City Name#3138, CBSA Name#3139]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enriched_df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d36f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
