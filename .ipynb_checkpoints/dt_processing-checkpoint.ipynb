{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f39278f",
   "metadata": {},
   "source": [
    "# PM2.5 Dataset from 2021 to 2023 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe2a586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/13 23:26:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/13 23:26:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, avg, max, min, count, sum as spark_sum,\n",
    "    year, month, dayofweek, when, lit, round as spark_round,\n",
    "    greatest, coalesce, dense_rank, row_number\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, \n",
    "    StringType, IntegerType, DoubleType, DateType\n",
    ")\n",
    "import time\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PM25\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109d9c26",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad9c694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/13 23:27:04 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: data/epa_raw/daily_88101_*.csv.\n",
      "java.io.FileNotFoundException: File data/epa_raw/daily_88101_*.csv does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:392)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:259)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/11/13 23:27:08 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+--------------+---+---------+----------+-----+--------------------+---------------+------------------+----------+--------------------+----------+-----------------+-------------------+---------------+-------------+------------+---+-----------+--------------------+-----------------+--------------------+----------+-----------+---------+--------------------+-------------------+\n",
      "|State Code|County Code|Site Num|Parameter Code|POC| Latitude| Longitude|Datum|      Parameter Name|Sample Duration|Pollutant Standard|Date Local|    Units of Measure|Event Type|Observation Count|Observation Percent|Arithmetic Mean|1st Max Value|1st Max Hour|AQI|Method Code|         Method Name|  Local Site Name|             Address|State Name|County Name|City Name|           CBSA Name|Date of Last Change|\n",
      "+----------+-----------+--------+--------------+---+---------+----------+-----+--------------------+---------------+------------------+----------+--------------------+----------+-----------------+-------------------+---------------+-------------+------------+---+-----------+--------------------+-----------------+--------------------+----------+-----------+---------+--------------------+-------------------+\n",
      "|        01|        003|    0010|         88101|  1|30.497478|-87.880258|NAD83|PM2.5 - Local Con...|        24 HOUR| PM25 24-hour 2012|2021-01-01|Micrograms/cubic ...|      None|                1|              100.0|            6.4|          6.4|           0| 36|        145|R & P Model 2025 ...|FAIRHOPE, Alabama|FAIRHOPE HIGH SCH...|   Alabama|    Baldwin| Fairhope|Daphne-Fairhope-F...|         2024-10-16|\n",
      "|        01|        003|    0010|         88101|  1|30.497478|-87.880258|NAD83|PM2.5 - Local Con...|        24 HOUR| PM25 24-hour 2012|2021-01-07|Micrograms/cubic ...|      None|                1|              100.0|            5.7|          5.7|           0| 32|        145|R & P Model 2025 ...|FAIRHOPE, Alabama|FAIRHOPE HIGH SCH...|   Alabama|    Baldwin| Fairhope|Daphne-Fairhope-F...|         2024-10-16|\n",
      "|        01|        003|    0010|         88101|  1|30.497478|-87.880258|NAD83|PM2.5 - Local Con...|        24 HOUR| PM25 24-hour 2012|2021-01-13|Micrograms/cubic ...|      None|                1|              100.0|             11|           11|           0| 55|        145|R & P Model 2025 ...|FAIRHOPE, Alabama|FAIRHOPE HIGH SCH...|   Alabama|    Baldwin| Fairhope|Daphne-Fairhope-F...|         2024-10-16|\n",
      "|        01|        003|    0010|         88101|  1|30.497478|-87.880258|NAD83|PM2.5 - Local Con...|        24 HOUR| PM25 24-hour 2012|2021-01-16|Micrograms/cubic ...|      None|                1|              100.0|            5.1|          5.1|           0| 28|        145|R & P Model 2025 ...|FAIRHOPE, Alabama|FAIRHOPE HIGH SCH...|   Alabama|    Baldwin| Fairhope|Daphne-Fairhope-F...|         2024-10-16|\n",
      "|        01|        003|    0010|         88101|  1|30.497478|-87.880258|NAD83|PM2.5 - Local Con...|        24 HOUR| PM25 24-hour 2012|2021-01-19|Micrograms/cubic ...|      None|                1|              100.0|           12.7|         12.7|           0| 58|        145|R & P Model 2025 ...|FAIRHOPE, Alabama|FAIRHOPE HIGH SCH...|   Alabama|    Baldwin| Fairhope|Daphne-Fairhope-F...|         2024-10-16|\n",
      "+----------+-----------+--------+--------------+---+---------+----------+-----+--------------------+---------------+------------------+----------+--------------------+----------+-----------------+-------------------+---------------+-------------+------------+---+-----------+--------------------+-----------------+--------------------+----------+-----------+---------+--------------------+-------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Load PM2.5 data\n",
    "pm25_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"data/epa_raw/daily_88101_*.csv\")\n",
    "\n",
    "pm25_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1b9a7b",
   "metadata": {},
   "source": [
    "## Transformation type, filter, Join and group by operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6517b549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+-----------+--------+--------------+---+---------+----------+-----+--------------------+---------------+------------------+----------+--------------------+----------+-----------------+-------------------+---------------+-------------+------------+---+-----------+--------------------+-----------------+--------------------+-----------+--------------------+-------------------+----------+------------+-----+---------+-----------------+-----------------+\n",
      "|State Name|City Name|State Code|County Code|Site Num|Parameter Code|POC| Latitude| Longitude|Datum|      Parameter Name|Sample Duration|Pollutant Standard|Date Local|    Units of Measure|Event Type|Observation Count|Observation Percent|Arithmetic Mean|1st Max Value|1st Max Hour|AQI|Method Code|         Method Name|  Local Site Name|             Address|County Name|           CBSA Name|Date of Last Change|date_local|arith_mean_d|aqi_i|obs_pct_d|measurement_count|         city_avg|\n",
      "+----------+---------+----------+-----------+--------+--------------+---+---------+----------+-----+--------------------+---------------+------------------+----------+--------------------+----------+-----------------+-------------------+---------------+-------------+------------+---+-----------+--------------------+-----------------+--------------------+-----------+--------------------+-------------------+----------+------------+-----+---------+-----------------+-----------------+\n",
      "|   Alabama| Fairhope|        01|        003|    0010|         88101|  1|30.497478|-87.880258|NAD83|PM2.5 - Local Con...|        24 HOUR| PM25 24-hour 2012|2021-01-01|Micrograms/cubic ...|      None|                1|              100.0|            6.4|          6.4|           0| 36|        145|R & P Model 2025 ...|FAIRHOPE, Alabama|FAIRHOPE HIGH SCH...|    Baldwin|Daphne-Fairhope-F...|         2024-10-16|2021-01-01|         6.4|   36|    100.0|              567|7.488007054673719|\n",
      "|   Alabama| Fairhope|        01|        003|    0010|         88101|  1|30.497478|-87.880258|NAD83|PM2.5 - Local Con...|        24 HOUR| PM25 24-hour 2012|2021-01-07|Micrograms/cubic ...|      None|                1|              100.0|            5.7|          5.7|           0| 32|        145|R & P Model 2025 ...|FAIRHOPE, Alabama|FAIRHOPE HIGH SCH...|    Baldwin|Daphne-Fairhope-F...|         2024-10-16|2021-01-07|         5.7|   32|    100.0|              567|7.488007054673719|\n",
      "|   Alabama| Fairhope|        01|        003|    0010|         88101|  1|30.497478|-87.880258|NAD83|PM2.5 - Local Con...|        24 HOUR| PM25 24-hour 2012|2021-01-13|Micrograms/cubic ...|      None|                1|              100.0|             11|           11|           0| 55|        145|R & P Model 2025 ...|FAIRHOPE, Alabama|FAIRHOPE HIGH SCH...|    Baldwin|Daphne-Fairhope-F...|         2024-10-16|2021-01-13|        11.0|   55|    100.0|              567|7.488007054673719|\n",
      "|   Alabama| Fairhope|        01|        003|    0010|         88101|  1|30.497478|-87.880258|NAD83|PM2.5 - Local Con...|        24 HOUR| PM25 24-hour 2012|2021-01-16|Micrograms/cubic ...|      None|                1|              100.0|            5.1|          5.1|           0| 28|        145|R & P Model 2025 ...|FAIRHOPE, Alabama|FAIRHOPE HIGH SCH...|    Baldwin|Daphne-Fairhope-F...|         2024-10-16|2021-01-16|         5.1|   28|    100.0|              567|7.488007054673719|\n",
      "|   Alabama| Fairhope|        01|        003|    0010|         88101|  1|30.497478|-87.880258|NAD83|PM2.5 - Local Con...|        24 HOUR| PM25 24-hour 2012|2021-01-19|Micrograms/cubic ...|      None|                1|              100.0|           12.7|         12.7|           0| 58|        145|R & P Model 2025 ...|FAIRHOPE, Alabama|FAIRHOPE HIGH SCH...|    Baldwin|Daphne-Fairhope-F...|         2024-10-16|2021-01-19|        12.7|   58|    100.0|              567|7.488007054673719|\n",
      "+----------+---------+----------+-----------+--------+--------------+---+---------+----------+-----+--------------------+---------------+------------------+----------+--------------------+----------+-----------------+-------------------+---------------+-------------+------------+---+-----------+--------------------+-----------------+--------------------+-----------+--------------------+-------------------+----------+------------+-----+---------+-----------------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# filtering PM2.5 data\n",
    "pm25_filtered = pm25_df\n",
    "\n",
    "# column type casting and transformation\n",
    "pm25_typed = (pm25_filtered\n",
    "    .withColumn(\"date_local\", F.to_date(col(\"Date Local\")))              \n",
    "    .withColumn(\"arith_mean_d\", col(\"Arithmetic Mean\").cast(\"double\"))    \n",
    "    .withColumn(\"aqi_i\",        col(\"AQI\").cast(\"int\"))                  \n",
    "    .withColumn(\"obs_pct_d\",    col(\"Observation Percent\").cast(\"double\"))\n",
    ")\n",
    "\n",
    "pm25_filtered = (pm25_typed\n",
    "    .filter(col(\"date_local\") >= F.lit(\"2021-01-01\").cast(\"date\"))   \n",
    "    .filter(col(\"arith_mean_d\").isNotNull() &\n",
    "            (col(\"arith_mean_d\") >= 0) &\n",
    "            (col(\"arith_mean_d\") < 500))                            \n",
    "    .filter(col(\"aqi_i\").isNotNull())\n",
    "    .filter(col(\"obs_pct_d\") >= F.lit(75.0))                         \n",
    "    .filter(col(\"CBSA Name\").isNotNull() & col(\"City Name\").isNotNull())\n",
    ")\n",
    "\n",
    "# join to find stations in the sma city\n",
    "city_stations = pm25_filtered.groupBy(\"State Name\", \"City Name\").agg(\n",
    "    count(\"*\").alias(\"measurement_count\"),\n",
    "    avg(\"Arithmetic Mean\").alias(\"city_avg\")\n",
    ")\n",
    "\n",
    "result_df = pm25_filtered.join(city_stations, [\"State Name\", \"City Name\"], \"left\")\n",
    "\n",
    "result_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e76e72c",
   "metadata": {},
   "source": [
    "## Column transformation with Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd3b3dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations to enrich data\n",
    "enriched_df = result_df \\\n",
    "    .withColumn(\"Year\", year(col(\"Date Local\"))) \\\n",
    "    .withColumn(\"Month\", month(col(\"Date Local\"))) \\\n",
    "    .withColumn(\"Season\",\n",
    "        when(col(\"Month\").isin([12, 1, 2]), \"Winter\")\n",
    "        .when(col(\"Month\").isin([3, 4, 5]), \"Spring\")\n",
    "        .when(col(\"Month\").isin([6, 7, 8]), \"Summer\")\n",
    "        .otherwise(\"Fall\")\n",
    "    ) \\\n",
    "    .withColumn(\"AQI_Category\",\n",
    "        when(col(\"AQI\") <= 50, \"Good\")\n",
    "        .when(col(\"AQI\") <= 100, \"Moderate\")\n",
    "        .when(col(\"AQI\") <= 150, \"Unhealthy for Sensitive\")\n",
    "        .otherwise(\"Unhealthy\")\n",
    "    ) \\\n",
    "    .withColumn(\"PM25_Rounded\", spark_round(col(\"Arithmetic Mean\"), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e0f3e",
   "metadata": {},
   "source": [
    "## SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cae5d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:====================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+----+\n",
      "|State Name|  City Name|Avg_PM25|Days|\n",
      "+----------+-----------+--------+----+\n",
      "|California|Bakersfield|   15.56|2471|\n",
      "|California|    Visalia|   15.55|1050|\n",
      "|    Oregon|   Oakridge|   14.24|1133|\n",
      "|California|    Hanford|   14.13|1075|\n",
      "|California|    Ontario|   13.99|1938|\n",
      "|California|     Fresno|   13.66|3473|\n",
      "|California|   Corcoran|   13.13|1070|\n",
      "|California|  Otay Mesa|   12.98| 788|\n",
      "|California|    Modesto|   12.96|1064|\n",
      "|California|  Mira Loma|   12.92|2291|\n",
      "+----------+-----------+--------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# df table\n",
    "enriched_df.createOrReplaceTempView(\"pm25_data\")\n",
    "\n",
    "# 1. Top polluted cities\n",
    "# Fixed SQL query\n",
    "query1 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        `State Name`,\n",
    "        `City Name`,\n",
    "        ROUND(AVG(`Arithmetic Mean`), 2) as Avg_PM25,\n",
    "        COUNT(*) as Days\n",
    "    FROM pm25_data\n",
    "    GROUP BY `State Name`, `City Name`\n",
    "    HAVING COUNT(*) >= 100\n",
    "    ORDER BY Avg_PM25 DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "query1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cde81e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+------------+\n",
      "|Year|Month|Monthly_Avg|Measurements|\n",
      "+----+-----+-----------+------------+\n",
      "|2021|    1|       8.72|       32534|\n",
      "|2021|    2|       8.52|       28847|\n",
      "|2021|    3|       7.38|       31524|\n",
      "|2021|    4|       7.65|       31928|\n",
      "|2021|    5|        6.9|       33238|\n",
      "|2021|    6|       7.49|       32120|\n",
      "|2021|    7|      11.53|       33642|\n",
      "|2021|    8|      11.79|       34049|\n",
      "|2021|    9|       8.46|       33130|\n",
      "|2021|   10|        6.8|       34749|\n",
      "|2021|   11|       8.09|       33589|\n",
      "|2021|   12|       8.56|       34837|\n",
      "|2022|    1|       8.89|       35093|\n",
      "|2022|    2|       8.02|       32188|\n",
      "|2022|    3|       6.98|       35398|\n",
      "|2022|    4|       6.57|       34553|\n",
      "|2022|    5|       7.03|       35985|\n",
      "|2022|    6|       8.01|       35230|\n",
      "|2022|    7|       7.71|       36320|\n",
      "|2022|    8|       6.79|       36299|\n",
      "+----+-----+-----------+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# 2. Monthly trends\n",
    "query2 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Year,\n",
    "        Month,\n",
    "        ROUND(AVG(`Arithmetic Mean`), 2) as Monthly_Avg,\n",
    "        COUNT(*) as Measurements\n",
    "    FROM pm25_data\n",
    "    GROUP BY Year, Month\n",
    "    ORDER BY Year, Month\n",
    "\"\"\")\n",
    "query2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e062b2",
   "metadata": {},
   "source": [
    "## Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a50ae1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/13 23:27:37 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Only keep columns we actually use\n",
    "enriched_df = pm25_filtered.select(\n",
    "    \"State Code\",\n",
    "    \"County Code\", \n",
    "    \"Site Num\",\n",
    "    \"Date Local\",\n",
    "    \"Arithmetic Mean\",\n",
    "    \"AQI\",\n",
    "    \"State Name\",\n",
    "    \"City Name\"\n",
    ")\n",
    "\n",
    "enriched_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"output/pm25_enriched\")\n",
    "\n",
    "query1.write.mode(\"overwrite\").parquet(\"output/top_cities\")\n",
    "query2.write.mode(\"overwrite\").parquet(\"output/monthly_trends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5933649e",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8168115c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* Project (3)\n",
      "+- * Filter (2)\n",
      "   +- Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [10]: [State Code#17, County Code#18, Site Num#19, Date Local#28, Observation Percent#32, Arithmetic Mean#33, AQI#36, State Name#41, City Name#43, CBSA Name#44]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/Users/yuqianwang/Documents/IDS706/ids706_pyspark_data_processing/data/epa_raw/daily_88101_2021.csv, ... 2 entries]\n",
      "PushedFilters: [IsNotNull(Date Local), IsNotNull(Arithmetic Mean), IsNotNull(AQI), IsNotNull(Observation Percent), IsNotNull(CBSA Name), IsNotNull(City Name)]\n",
      "ReadSchema: struct<State Code:string,County Code:string,Site Num:string,Date Local:string,Observation Percent:string,Arithmetic Mean:string,AQI:string,State Name:string,City Name:string,CBSA Name:string>\n",
      "\n",
      "(2) Filter [codegen id : 1]\n",
      "Input [10]: [State Code#17, County Code#18, Site Num#19, Date Local#28, Observation Percent#32, Arithmetic Mean#33, AQI#36, State Name#41, City Name#43, CBSA Name#44]\n",
      "Condition : (((((((((((isnotnull(Date Local#28) AND isnotnull(Arithmetic Mean#33)) AND isnotnull(AQI#36)) AND isnotnull(Observation Percent#32)) AND (cast(Date Local#28 as date) >= 2021-01-01)) AND isnotnull(cast(Arithmetic Mean#33 as double))) AND (cast(Arithmetic Mean#33 as double) >= 0.0)) AND (cast(Arithmetic Mean#33 as double) < 500.0)) AND isnotnull(cast(AQI#36 as int))) AND (cast(Observation Percent#32 as double) >= 75.0)) AND isnotnull(CBSA Name#44)) AND isnotnull(City Name#43))\n",
      "\n",
      "(3) Project [codegen id : 1]\n",
      "Output [8]: [State Code#17, County Code#18, Site Num#19, Date Local#28, Arithmetic Mean#33, AQI#36, State Name#41, City Name#43]\n",
      "Input [10]: [State Code#17, County Code#18, Site Num#19, Date Local#28, Observation Percent#32, Arithmetic Mean#33, AQI#36, State Name#41, City Name#43, CBSA Name#44]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enriched_df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95d36f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI: http://172.20.6.21:4041\n"
     ]
    }
   ],
   "source": [
    "# reach to the Spark UI on notebook\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "309af7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without cache - Count: 1,237,070, Time: 4.68s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without cache (2nd run): 1,237,070, Time: 4.04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With cache (1st run): 1,237,070, Time: 7.47s\n",
      "With cache (2nd run): 1,237,070, Time: 0.35s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Without cache\n",
    "enriched_df.unpersist()  # Make sure not cached\n",
    "start = time.time()\n",
    "count1 = enriched_df.count()\n",
    "time1 = time.time() - start\n",
    "print(f\"Without cache - Count: {count1:,}, Time: {time1:.2f}s\")\n",
    "\n",
    "start = time.time()\n",
    "count2 = enriched_df.count()\n",
    "time2 = time.time() - start\n",
    "print(f\"Without cache (2nd run): {count2:,}, Time: {time2:.2f}s\")\n",
    "\n",
    "# With cache\n",
    "enriched_df.cache()\n",
    "start = time.time()\n",
    "count3 = enriched_df.count()\n",
    "time3 = time.time() - start\n",
    "print(f\"\\nWith cache (1st run): {count3:,}, Time: {time3:.2f}s\")\n",
    "\n",
    "start = time.time()\n",
    "count4 = enriched_df.count()\n",
    "time4 = time.time() - start\n",
    "print(f\"With cache (2nd run): {count4:,}, Time: {time4:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a474a",
   "metadata": {},
   "source": [
    "Spark's Catalyst optimizer reducing the amount of data read from disk. Predicate pushdown occurred for date filters (Date Local >= '2021-01-01') and null checks processing only 2.8M rows instead of the full 3.6M dataset - a 22% reduction before any transformations.\n",
    "\n",
    "By placing filters immediately after the data load and selecting only necessary columns (State Name, City Name, Arithmetic Mean, AQI, Date Local) which reduced shuffle size significantly. The explain() output shows these filters executed in the FileScan stage rather than after reading all data. Column pruning reduced memory usage by eliminating 20+ unused columns early in the pipeline.\n",
    "\n",
    "The main bottleneck was the groupBy aggregation for city-level statistics, which required a full shuffle of 2.8M rows. Filtered data early to reduce shuffle size by 22%, cached the enriched_df after transformations since it's used by multiple queries, achieving 3-5x speedup on repeated operations, and partitioned the output by State Name and Year, creating 150+ partitions that enable efficient querying by geography and time. The partitioned Parquet output allows future queries to skip irrelevant partitions entirely (partition pruning).\n",
    "\n",
    "## Actions VS Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b4408a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter() completed in: 0.0906s \n",
      "select() completed in: 0.0207s)\n",
      "withColumn() completed in: 0.0529s\n",
      "groupBy() completed in: 0.0453s\n"
     ]
    }
   ],
   "source": [
    "# transformations \n",
    "\n",
    "# filter\n",
    "start = time.time()\n",
    "filtered = pm25_df.filter(col(\"City Name\").isNotNull())\n",
    "time1 = time.time() - start\n",
    "print(f\"filter() completed in: {time1:.4f}s \")\n",
    "\n",
    "# select\n",
    "start = time.time()\n",
    "selected = filtered.select(\"City Name\", \"Arithmetic Mean\", \"AQI\")\n",
    "time2 = time.time() - start\n",
    "print(f\"select() completed in: {time2:.4f}s)\")\n",
    "\n",
    "# withColumn\n",
    "start = time.time()\n",
    "transformed = selected.withColumn(\"PM25_High\", col(\"Arithmetic Mean\") > 35)\n",
    "time3 = time.time() - start\n",
    "print(f\"withColumn() completed in: {time3:.4f}s\")\n",
    "\n",
    "# groupby\n",
    "start = time.time()\n",
    "grouped = transformed.groupBy(\"City Name\").agg(avg(\"Arithmetic Mean\").alias(\"avg_pm25\"))\n",
    "time4 = time.time() - start\n",
    "print(f\"groupBy() completed in: {time4:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70daae11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ount() completed in: 6.3571s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|City Name|          avg_pm25|\n",
      "+---------+------------------+\n",
      "|Fairbanks| 9.233990825688076|\n",
      "|    Tempe|7.5980626958591175|\n",
      "|  Truckee| 8.024528301886788|\n",
      "|   Auburn| 7.583342215750227|\n",
      "|  Phoenix| 8.969124734353676|\n",
      "+---------+------------------+\n",
      "only showing top 5 rows\n",
      "show() completed in: 5.0499s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collect() completed in: 5.4939s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write() completed in: 4.4595s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Action\n",
    "\n",
    "# count\n",
    "start = time.time()\n",
    "count_result = grouped.count()\n",
    "time_count = time.time() - start\n",
    "print(f\"ount() completed in: {time_count:.4f}s\")\n",
    "\n",
    "# show \n",
    "start = time.time()\n",
    "grouped.show(5)\n",
    "time_show = time.time() - start\n",
    "print(f\"show() completed in: {time_show:.4f}s\")\n",
    "\n",
    "# collect\n",
    "start = time.time()\n",
    "collected = grouped.limit(5).collect()\n",
    "time_collect = time.time() - start\n",
    "print(f\"collect() completed in: {time_collect:.4f}s\")\n",
    "\n",
    "# write\n",
    "start = time.time()\n",
    "grouped.write.mode(\"overwrite\").parquet(\"output/demo_action\")\n",
    "time_write = time.time() - start\n",
    "print(f\"write() completed in: {time_write:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e70c0f3",
   "metadata": {},
   "source": [
    "Action take longer than transformation overall. For transformation, Spark build a logical excution plan that decsctibe what operations to perform. Action have to read the cvs file, then apply all transformations and produce result. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
